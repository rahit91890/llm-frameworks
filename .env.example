# ==================================
# LLM Frameworks Environment Template
# ==================================
# Copy this file to .env and fill in your actual values
# Command: cp .env.example .env
#
# IMPORTANT: Never commit your .env file to version control!
# It contains sensitive API keys and secrets.

# ==================================
# OpenAI Configuration
# ==================================
# Get your API key from: https://platform.openai.com/
OPENAI_API_KEY=your_openai_api_key_here

# Model Selection
# Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo-preview, gpt-4o, etc.
OPENAI_MODEL=gpt-3.5-turbo

# Optional: Organization ID (if you have multiple organizations)
# OPENAI_ORGANIZATION=your_org_id_here

# ==================================
# HuggingFace Configuration
# ==================================
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_huggingface_token_here

# Model Selection
# Options: gpt2, facebook/opt-350m, EleutherAI/gpt-neo-2.7B, etc.
# Browse models at: https://huggingface.co/models
HF_MODEL=gpt2

# Optional: Use GPU if available
# HF_DEVICE=cuda

# ==================================
# Provider Selection
# ==================================
# Default LLM provider to use
# Options: openai, huggingface
DEFAULT_PROVIDER=openai

# ==================================
# Flask Web Interface Configuration
# ==================================
# Environment mode
# Options: development, production
FLASK_ENV=development

# Port number for Flask web server
FLASK_PORT=5000

# Secret key for Flask sessions (generate a random string for production)
FLASK_SECRET_KEY=your_secret_key_here_change_in_production

# ==================================
# Logging Configuration
# ==================================
# Log level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (optional)
# LOG_FILE=logs/llm_frameworks.log

# ==================================
# Advanced Settings (Optional)
# ==================================
# Maximum tokens for completions
MAX_TOKENS=150

# Temperature for text generation (0.0 - 2.0)
# Lower values = more deterministic, Higher values = more creative
TEMPERATURE=0.7

# Request timeout in seconds
REQUEST_TIMEOUT=30
